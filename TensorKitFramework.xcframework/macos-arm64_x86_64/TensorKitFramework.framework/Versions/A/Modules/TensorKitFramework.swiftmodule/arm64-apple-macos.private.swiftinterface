// swift-interface-format-version: 1.0
// swift-compiler-version: Apple Swift version 5.10 (swiftlang-5.10.0.13 clang-1500.3.9.4)
// swift-module-flags: -target arm64-apple-macos14.2 -enable-objc-interop -enable-library-evolution -swift-version 5 -enforce-exclusivity=checked -O -enable-bare-slash-regex -module-name TensorKitFramework
import Accelerate
import Foundation
import Swift
@_exported import TensorKitFramework
import _Concurrency
import _StringProcessing
import _SwiftConcurrencyShims
public enum TensorInitialization : Swift.Codable {
  case ones
  case zeros
  case uniform
  case xavier_glorot
  case he
  case random
  case random_small
  case empty
  public static func == (a: TensorKitFramework.TensorInitialization, b: TensorKitFramework.TensorInitialization) -> Swift.Bool
  public func hash(into hasher: inout Swift.Hasher)
  public func encode(to encoder: any Swift.Encoder) throws
  public var hashValue: Swift.Int {
    get
  }
  public init(from decoder: any Swift.Decoder) throws
}
public class Tensor<T> : Swift.Codable where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable {
  public var shape: [Swift.Int]
  public var data: [T]
  public var operation: Swift.String?
  public var parents: [(TensorKitFramework.Tensor<T>, (TensorKitFramework.Tensor<T>) -> [T])]
  public var gradient: [T]?
  public var dataSize: Swift.Int {
    get
  }
  public init(_ input: [[[T]]], calculate_grad: Swift.Bool = false)
  public init(_ input: [[T]], calculate_grad: Swift.Bool = false)
  public init(_ input: [T], shape: [Swift.Int], calculate_grad: Swift.Bool = false)
  public init(_ input: T, calculate_grad: Swift.Bool = false)
  public init(_ initializer: TensorKitFramework.TensorInitialization = .zeros, shape: [Swift.Int], calculate_grad: Swift.Bool = false)
  public func encode(to encoder: any Swift.Encoder) throws
  required public init(from decoder: any Swift.Decoder) throws
  public func sum(along dimension: Swift.Int) -> TensorKitFramework.Tensor<T>
  public func sum(along: [Swift.Int]) -> TensorKitFramework.Tensor<T>
  public func sum() -> TensorKitFramework.Tensor<T>
  public func reshape(to newDimensions: [Swift.Int]) -> TensorKitFramework.Tensor<T>
  public func expand(to targetDimensions: [Swift.Int]) -> TensorKitFramework.Tensor<T>
  public func indexToFlatIndex(_ indices: [Swift.Int]) -> Swift.Int
  public func backward(_ grad: [T] = Array(repeating: 1.0, count: 0), printSteps: Swift.Bool = false)
  @objc deinit
}
public func selectShape(from shape: [Swift.Int], using indices: [Swift.Int]) -> [Swift.Int]
public func generateStrides(_ shape: [Swift.Int]) -> [Swift.Int]
public func calculateIndex(strides: [Swift.Int], index: [Swift.Int]) -> Swift.Int
public func sum<T>(_ input: [T], shape: [Swift.Int], along: Swift.Int) -> [T] where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func flattenedIndex(of index: Swift.Int, withShape newShape: [Swift.Int], along dimension: Swift.Int, innerIndex: Swift.Int, originalShape: [Swift.Int]) -> Swift.Int
public func sum<T>(_ data: [T], shape: [Swift.Int], along: [Swift.Int]) -> [T] where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func sum<T>(_ data: [T], shape: [Swift.Int], to: [Swift.Int]) -> [T] where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func sum<T>(_ data: [T], shape: [Swift.Int]) -> T where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func mergeShapes(_ a: [Swift.Int], _ b: [Swift.Int]) -> [Swift.Int]
public func multiply<T>(_ input: [T], s: T) -> [T] where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func divide<T>(_ input: [T], s: T) -> [T] where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func inverseDivide<T>(_ input: [T], s: T) -> [T] where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func matrixMultiply<T>(_ a: [T], _ b: [T], aShape: [Swift.Int], bShape: [Swift.Int]) -> [T] where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func transpose<T>(_ input: [T], shape: [Swift.Int]) -> [T] where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public func transpose<T>(_ input: TensorKitFramework.Tensor<T>) -> TensorKitFramework.Tensor<T> where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable
public class TestTensor<T> : Swift.Codable where T : Swift.BinaryFloatingPoint, T : Swift.Decodable, T : Swift.Encodable {
  public var shape: [Swift.Int]
  public var data: [T]
  public var operation: Swift.String?
  public var parents: [(TensorKitFramework.TestTensor<T>, (TensorKitFramework.TestTensor<T>) -> [T])]
  public var gradient: [T]?
  public var dataSize: Swift.Int {
    get
  }
  public init(_ input: [[[T]]], calculate_grad: Swift.Bool = false)
  public init(_ input: [[T]], calculate_grad: Swift.Bool = false)
  public init(_ input: [T], shape: [Swift.Int], calculate_grad: Swift.Bool = false)
  public init(_ input: T, calculate_grad: Swift.Bool = false)
  public init(_ initializer: TensorKitFramework.TensorInitialization = .zeros, shape: [Swift.Int], calculate_grad: Swift.Bool = false)
  public func encode(to encoder: any Swift.Encoder) throws
  required public init(from decoder: any Swift.Decoder) throws
  public enum CodingKeys : Swift.CodingKey {
    case data
    case shape
    case operation
    case gradient
    public static func == (a: TensorKitFramework.TestTensor<T>.CodingKeys, b: TensorKitFramework.TestTensor<T>.CodingKeys) -> Swift.Bool
    public func hash(into hasher: inout Swift.Hasher)
    public init?(stringValue: Swift.String)
    public init?(intValue: Swift.Int)
    public var hashValue: Swift.Int {
      get
    }
    public var intValue: Swift.Int? {
      get
    }
    public var stringValue: Swift.String {
      get
    }
  }
  public func backward(_ grad: [T] = Array(repeating: 1.0, count: 0), printSteps: Swift.Bool = false)
  @objc deinit
}
public class TestingClass {
  public init(hiddenValue: Swift.Int, PublicValue: Swift.Int)
  public var PublicValue: Swift.Int
  public func PshowHiddenValue() -> Swift.Int
  @objc deinit
}
public class example {
  public var name: Swift.String
  public var data: Foundation.Date
  public init(name: Swift.String)
  public func printInfo()
  @objc deinit
}
public var id: Swift.Int
public func createRandomExample() -> TensorKitFramework.example
infix operator ** : MultiplicationPrecedence
extension Swift.Array {
  public func inserting(_ newElement: Element, at i: Swift.Int) -> Swift.Array<Element>
}
public var testingValue: Swift.Double
public func softmax<T>(_ x: [T]) -> [T] where T : Swift.BinaryFloatingPoint
public func ReLU<T>(_ x: T) -> T where T : Swift.BinaryFloatingPoint
public func Sigmoid<T>(_ x: T) -> T where T : Swift.BinaryFloatingPoint
public func Tanh<T>(_ x: T) -> T where T : Swift.BinaryFloatingPoint
public func LeakyReLU<T>(_ x: T, alpha: T = 0.01) -> T where T : Swift.BinaryFloatingPoint
extension TensorKitFramework.TensorInitialization : Swift.Equatable {}
extension TensorKitFramework.TensorInitialization : Swift.Hashable {}
extension TensorKitFramework.TestTensor.CodingKeys : Swift.Equatable {}
extension TensorKitFramework.TestTensor.CodingKeys : Swift.Hashable {}
